# -*- coding: utf-8 -*-
"""airw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-jyKCz0W_6RDJm09speYUeMdffME9UEA
"""

#imported all needed libraries
import nltk
import pandas as pd
from nltk.tokenize import sent_tokenize,word_tokenize
from nltk.corpus import stopwords

nltk.download('punkt')

df = pd.read_csv("train.csv")

df.head()



"""Tokenization of tweet to sentence"""

tweetText = df["text"]

tweetText = tweetText.apply(sent_tokenize)
tweetText.head()

tweetText[1]

tweetText[2]



"""Tokenization of tweet to words"""

tweetWords = df["text"]

tweetWords = tweetWords.apply(word_tokenize)
tweetWords.head()

tweetWords[1]



"""Remove stopwords in each tweet"""

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

stop_words

df["text"] = df["text"].apply(word_tokenize)
df["text"].head()

print(type(df["text"]))

df["text"] = df["text"].apply(lambda words: [word for word in words if word not in stop_words])

df.head(5)

